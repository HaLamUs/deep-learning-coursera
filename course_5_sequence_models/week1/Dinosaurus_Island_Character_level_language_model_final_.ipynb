{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Dinosaurus Island -- Character level language model final .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhFOPSpcKalO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from utils import *\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmjgvNbvKbNM",
        "colab_type": "code",
        "outputId": "ba783626-9772-4e5a-c0a2-3353b332d555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "data = open('dinos.txt', 'r').read()\n",
        "data= data.lower()\n",
        "chars = list(set(data))\n",
        "print(sorted(chars))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "There are 19909 total characters and 27 unique characters in your data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIiQ_JCpW9em",
        "colab_type": "code",
        "outputId": "c28b9a40-8f9c-45dd-9f56-db7650f344fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
        "print(char_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECnR7607XFzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### GRADED FUNCTION: clip\n",
        "\n",
        "def clip(gradients, maxValue):\n",
        "    '''\n",
        "    Clips the gradients' values between minimum and maximum.\n",
        "    \n",
        "    Arguments:\n",
        "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
        "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
        "    \n",
        "    Returns: \n",
        "    gradients -- a dictionary with the clipped gradients.\n",
        "    '''\n",
        "    \n",
        "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
        "   \n",
        "    ### START CODE HERE ###\n",
        "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
        "    for value in [dWax, dWaa, dWya, db, dby]:\n",
        "      np.clip(value, -maxValue, maxValue, out=value)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A3GEsfpZmr1",
        "colab_type": "code",
        "outputId": "d3151d84-be50-4e4a-d207-95dc47e46456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "np.random.seed(3)\n",
        "dWax = np.random.randn(5,3)*10\n",
        "dWaa = np.random.randn(5,5)*10\n",
        "dWya = np.random.randn(2,5)*10\n",
        "db = np.random.randn(5,1)*10\n",
        "dby = np.random.randn(2,1)*10\n",
        "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "gradients = clip(gradients, 10)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dWaa\"][1][2] = 10.0\n",
            "gradients[\"dWax\"][3][1] = -10.0\n",
            "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
            "gradients[\"db\"][4] = [10.]\n",
            "gradients[\"dby\"][1] = [8.45833407]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niFB_67JdUZc",
        "colab_type": "code",
        "outputId": "36ef8672-af5d-46d7-892a-eaf84d53bf6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print(np.zeros(3) 1)\n",
        "a = np.zeros(10)\n",
        "x = [0 if i !=0 else 1 for i in range(10)]\n",
        "#np.put(a,5,1)\n",
        "# print(x)\n",
        "index = np.random.choice(5, 1, p=[0.1, 0, 0.3, 0.6, 0])#np.random.choice(5, 3)#np.random.choice(27,1, p = 0.16)\n",
        "print(index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKEyXKu1l5L_",
        "colab_type": "code",
        "outputId": "9576b207-1bbb-4f07-c21e-2595e4ce1275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "x = np.zeros((11, 1))\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv7fksMbljgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: sample\n",
        "\n",
        "def sample(parameters, char_to_ix, seed):\n",
        "    \"\"\"\n",
        "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
        "    char_to_ix -- python dictionary mapping each character to an index.\n",
        "    seed -- used for grading purposes. Do not worry about it.\n",
        "\n",
        "    Returns:\n",
        "    indices -- a list of length n containing the indices of the sampled characters.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
        "    x = np.zeros((vocab_size, 1)) # vector cột or default numpy array là vậy \n",
        "\n",
        "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    \n",
        "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
        "    indices = []\n",
        "    \n",
        "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
        "    idx = -1 \n",
        "    \n",
        "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
        "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
        "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
        "    counter = 0\n",
        "    test_y = 0\n",
        "    while (counter < 50 and idx != char_to_ix[\"\\n\"]):\n",
        "\n",
        "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
        "        a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
        "        z_next = np.dot(Wya, a_next) + by \n",
        "        y_pre = softmax(z_next) # y prediction, y này nó trả về 1 list đầy đủ xác suất luôn \n",
        "        test_y = y_pre  \n",
        "        \n",
        "        # for grading purposes\n",
        "        np.random.seed(counter+seed)\n",
        "        \n",
        "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
        "        index = np.random.choice(list(range(vocab_size)), p = y_pre.ravel())\n",
        "\n",
        "        # Append the index to \"indices\"\n",
        "        indices.append(index)\n",
        "        \n",
        "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[index] = 1 \n",
        "        \n",
        "        # Update \"a_prev\" to be \"a\"\n",
        "        a_prev = a_next \n",
        "        \n",
        "        # for grading purposes\n",
        "        seed += 1\n",
        "        counter +=1\n",
        "        idx = index \n",
        "\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "    # print(test_y.shape)\n",
        "    return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ny1otdiI_S",
        "colab_type": "code",
        "outputId": "90d9dce6-3b03-4d12-ee84-6738de981ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "np.random.seed(2)\n",
        "_, n_a = 20, 100\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "\n",
        "\n",
        "indices = sample(parameters, char_to_ix, 0)\n",
        "print(\"Sampling:\")\n",
        "print(\"list of sampled indices:\", indices, '\\n')\n",
        "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27, 1)\n",
            "Sampling:\n",
            "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 13, 24, 0] \n",
            "\n",
            "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'm', 'x', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ4oC-xKuz5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Execute one step of the optimization to train the model.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
        "    a_prev -- previous hidden state.\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    learning_rate -- learning rate for the model.\n",
        "    \n",
        "    Returns:\n",
        "    loss -- value of the loss function (cross-entropy)\n",
        "    gradients -- python dictionary containing:\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
        "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
        "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
        "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Forward propagate through time (≈1 line)\n",
        "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
        "    \n",
        "    # Backpropagate through time (≈1 line)\n",
        "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
        "    \n",
        "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
        "    gradients = clip(gradients, 5)\n",
        "    \n",
        "    # Update parameters (≈1 line)\n",
        "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return loss, gradients, a[len(X)-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43JoHaHjx3Mb",
        "colab_type": "code",
        "outputId": "d4ec1b61-b739-405d-b455-1e941cc2d088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "vocab_size, n_a = 27, 100\n",
        "a_prev = np.random.randn(n_a, 1)\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "X = [12,3,5,11,22,3]\n",
        "Y = [4,14,11,22,25, 26]\n",
        "\n",
        "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
        "print(\"Loss =\", loss)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
        "print(\"a_last[4] =\", a_last[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss = 126.50397572165369\n",
            "gradients[\"dWaa\"][1][2] = 0.19470931534720928\n",
            "np.argmax(gradients[\"dWax\"]) = 93\n",
            "gradients[\"dWya\"][1][2] = -0.007773876032003706\n",
            "gradients[\"db\"][4] = [-0.06809825]\n",
            "gradients[\"dby\"][1] = [0.01538192]\n",
            "a_last[4] = [-1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W145fG9DyrfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
        "    \"\"\"\n",
        "    Trains the model and generates dinosaur names. \n",
        "    \n",
        "    Arguments:\n",
        "    data -- text corpus\n",
        "    ix_to_char -- dictionary that maps the index to a character\n",
        "    char_to_ix -- dictionary that maps a character to an index\n",
        "    num_iterations -- number of iterations to train the model for\n",
        "    n_a -- number of units of the RNN cell\n",
        "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
        "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- learned parameters\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve n_x and n_y from vocab_size\n",
        "    n_x, n_y = vocab_size, vocab_size\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
        "    \n",
        "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "    \n",
        "    # Build list of all dinosaur names (training examples).\n",
        "    with open(\"dinos.txt\") as f:\n",
        "        examples = f.readlines()\n",
        "    examples = [x.lower().strip() for x in examples]\n",
        "    \n",
        "    # Shuffle list of all dinosaur names\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "    \n",
        "    # Initialize the hidden state of your LSTM\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    \n",
        "    # Optimization loop\n",
        "    for j in range(num_iterations):\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
        "        X = [None] + [char_to_ix[ch] for ch in examples[j % len(examples)]] \n",
        "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
        "        \n",
        "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
        "        # Choose a learning rate of 0.01\n",
        "        curr_loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
        "        loss = smooth(loss, curr_loss)\n",
        "\n",
        "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
        "        if j % 2000 == 0:\n",
        "            \n",
        "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
        "            \n",
        "            # The number of dinosaur names to print\n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "                \n",
        "                # Sample indices and print them\n",
        "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
        "                print_sample(sampled_indices, ix_to_char)\n",
        "                \n",
        "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
        "      \n",
        "            print('\\n')\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyRC41nRzwAD",
        "colab_type": "code",
        "outputId": "2fd08777-db51-4751-87e3-06b8e65eef9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "parameters = model(data, ix_to_char, char_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0, Loss: 23.087336\n",
            "\n",
            "(27, 1)\n",
            "Nkzxwtdmfqoeyhsqwasjkjvu\n",
            "(27, 1)\n",
            "Kneb\n",
            "(27, 1)\n",
            "Kzxwtdmfqoeyhsqwasjkjvu\n",
            "(27, 1)\n",
            "Neb\n",
            "(27, 1)\n",
            "Zxwtdmfqoeyhsqwasjkjvu\n",
            "(27, 1)\n",
            "Eb\n",
            "(27, 1)\n",
            "Xwtdmfqoeyhsqwasjkjvu\n",
            "\n",
            "\n",
            "Iteration: 2000, Loss: 27.336073\n",
            "\n",
            "(27, 1)\n",
            "Mhystolonoraurus\n",
            "(27, 1)\n",
            "Imecalosaurus\n",
            "(27, 1)\n",
            "Kystolonoraurus\n",
            "(27, 1)\n",
            "Macalpsalasrus\n",
            "(27, 1)\n",
            "Yusogoloraurus\n",
            "(27, 1)\n",
            "Dabersahaus\n",
            "(27, 1)\n",
            "Torangosaurus\n",
            "\n",
            "\n",
            "Iteration: 4000, Loss: 25.118910\n",
            "\n",
            "(27, 1)\n",
            "Mevrosaurus\n",
            "(27, 1)\n",
            "Hifaagosaurus\n",
            "(27, 1)\n",
            "Ivusninesaurus\n",
            "(27, 1)\n",
            "Macalosaurus\n",
            "(27, 1)\n",
            "Xusolonosaurus\n",
            "(27, 1)\n",
            "Cabcosaurus\n",
            "(27, 1)\n",
            "Torangosaurus\n",
            "\n",
            "\n",
            "Iteration: 6000, Loss: 23.797897\n",
            "\n",
            "(27, 1)\n",
            "Niwrosaurus\n",
            "(27, 1)\n",
            "Kieealosaurus\n",
            "(27, 1)\n",
            "Kustolonosaurus\n",
            "(27, 1)\n",
            "Necaisieia\n",
            "(27, 1)\n",
            "Xustenlosaurus\n",
            "(27, 1)\n",
            "Daakosaurus\n",
            "(27, 1)\n",
            "Troflesaurus\n",
            "\n",
            "\n",
            "Iteration: 8000, Loss: 23.276764\n",
            "\n",
            "(27, 1)\n",
            "Mewussarisaurus\n",
            "(27, 1)\n",
            "Ingbalosaurus\n",
            "(27, 1)\n",
            "Iususaurus\n",
            "(27, 1)\n",
            "Macalosaurus\n",
            "(27, 1)\n",
            "Xustariosaurus\n",
            "(27, 1)\n",
            "Daadron\n",
            "(27, 1)\n",
            "Trodolophus\n",
            "\n",
            "\n",
            "Iteration: 10000, Loss: 23.055204\n",
            "\n",
            "(27, 1)\n",
            "Nixtrodon\n",
            "(27, 1)\n",
            "Kolaaeropa\n",
            "(27, 1)\n",
            "Lustreodor\n",
            "(27, 1)\n",
            "Ndaaeron\n",
            "(27, 1)\n",
            "Xustenatikynisanosaurun\n",
            "(27, 1)\n",
            "Daaerona\n",
            "(27, 1)\n",
            "Trocheoraverater\n",
            "\n",
            "\n",
            "Iteration: 12000, Loss: 22.395661\n",
            "\n",
            "(27, 1)\n",
            "Mexustanesaurus\n",
            "(27, 1)\n",
            "Jokaagosaurus\n",
            "(27, 1)\n",
            "Kustodonasaurus\n",
            "(27, 1)\n",
            "Macagosaurus\n",
            "(27, 1)\n",
            "Yussanesaurus\n",
            "(27, 1)\n",
            "Daaepticanrasaurus\n",
            "(27, 1)\n",
            "Tosaonosaurus\n",
            "\n",
            "\n",
            "Iteration: 14000, Loss: 22.370564\n",
            "\n",
            "(27, 1)\n",
            "Mawtosaurus\n",
            "(27, 1)\n",
            "Hiabaisora\n",
            "(27, 1)\n",
            "Iurqolonothus\n",
            "(27, 1)\n",
            "Macairoracous\n",
            "(27, 1)\n",
            "Yusterips\n",
            "(27, 1)\n",
            "Cabbrona\n",
            "(27, 1)\n",
            "Trochinoeus\n",
            "\n",
            "\n",
            "Iteration: 16000, Loss: 22.329815\n",
            "\n",
            "(27, 1)\n",
            "Mdussueolosaurus\n",
            "(27, 1)\n",
            "Hiadagropeclus\n",
            "(27, 1)\n",
            "Iutosaurus\n",
            "(27, 1)\n",
            "Macalosaurus\n",
            "(27, 1)\n",
            "Xuruanchulurus\n",
            "(27, 1)\n",
            "Caadosaurus\n",
            "(27, 1)\n",
            "Toranigontesaurus\n",
            "\n",
            "\n",
            "Iteration: 18000, Loss: 21.936210\n",
            "\n",
            "(27, 1)\n",
            "Mewtosaurus\n",
            "(27, 1)\n",
            "Kigaakosaurus\n",
            "(27, 1)\n",
            "Lytrsaurus\n",
            "(27, 1)\n",
            "Madaerosaurus\n",
            "(27, 1)\n",
            "Yussasaurus\n",
            "(27, 1)\n",
            "Daalosaurus\n",
            "(27, 1)\n",
            "Troengosaurus\n",
            "\n",
            "\n",
            "Iteration: 20000, Loss: 21.963944\n",
            "\n",
            "(27, 1)\n",
            "Mattosaurus\n",
            "(27, 1)\n",
            "Kolaacisaurus\n",
            "(27, 1)\n",
            "Kustychanohuchus\n",
            "(27, 1)\n",
            "Macagprodensaurus\n",
            "(27, 1)\n",
            "Yuskandondylusaurus\n",
            "(27, 1)\n",
            "Daahurhaenosaurus\n",
            "(27, 1)\n",
            "Trrarjkonychus\n",
            "\n",
            "\n",
            "Iteration: 22000, Loss: 21.862372\n",
            "\n",
            "(27, 1)\n",
            "Mexurodonioitas\n",
            "(27, 1)\n",
            "Klecaeropeclysaurus\n",
            "(27, 1)\n",
            "Kustrephopeurus\n",
            "(27, 1)\n",
            "Macaeosaurus\n",
            "(27, 1)\n",
            "Yuprephopeurus\n",
            "(27, 1)\n",
            "Eiadosaurus\n",
            "(27, 1)\n",
            "Trocheorax\n",
            "\n",
            "\n",
            "Iteration: 24000, Loss: 21.768626\n",
            "\n",
            "(27, 1)\n",
            "Lixrrondesaurus\n",
            "(27, 1)\n",
            "Gocacerandaus\n",
            "(27, 1)\n",
            "Hystrionionus\n",
            "(27, 1)\n",
            "Ldaachsaurus\n",
            "(27, 1)\n",
            "Yussasaurus\n",
            "(27, 1)\n",
            "Caacisaurus\n",
            "(27, 1)\n",
            "Troceratops\n",
            "\n",
            "\n",
            "Iteration: 26000, Loss: 21.722073\n",
            "\n",
            "(27, 1)\n",
            "Mevtosaurus\n",
            "(27, 1)\n",
            "Hicabarten\n",
            "(27, 1)\n",
            "Itosedomnikus\n",
            "(27, 1)\n",
            "Macadrolcerotops\n",
            "(27, 1)\n",
            "Xunocheptitan\n",
            "(27, 1)\n",
            "Cabbosaurus\n",
            "(27, 1)\n",
            "Trochinneurus\n",
            "\n",
            "\n",
            "Iteration: 28000, Loss: 21.705440\n",
            "\n",
            "(27, 1)\n",
            "Mettrodon\n",
            "(27, 1)\n",
            "Hicaadropderylus\n",
            "(27, 1)\n",
            "Itrosaurus\n",
            "(27, 1)\n",
            "Macadrrceidrosaurus\n",
            "(27, 1)\n",
            "Yutichini\n",
            "(27, 1)\n",
            "Cacaros\n",
            "(27, 1)\n",
            "Trocheosaurus\n",
            "\n",
            "\n",
            "Iteration: 30000, Loss: 21.741199\n",
            "\n",
            "(27, 1)\n",
            "Matstros\n",
            "(27, 1)\n",
            "Joicachyel\n",
            "(27, 1)\n",
            "Kustramesausus\n",
            "(27, 1)\n",
            "Macadosaurus\n",
            "(27, 1)\n",
            "Xutibidosaurus\n",
            "(27, 1)\n",
            "Cachysaurus\n",
            "(27, 1)\n",
            "Trocodon\n",
            "\n",
            "\n",
            "Iteration: 32000, Loss: 21.424200\n",
            "\n",
            "(27, 1)\n",
            "Mevuosaurus\n",
            "(27, 1)\n",
            "Hualamiracbus\n",
            "(27, 1)\n",
            "Juounaraptos\n",
            "(27, 1)\n",
            "Macalnorachus\n",
            "(27, 1)\n",
            "Yupodomliauraptar\n",
            "(27, 1)\n",
            "Cabarona\n",
            "(27, 1)\n",
            "Tosaurus\n",
            "\n",
            "\n",
            "Iteration: 34000, Loss: 21.436294\n",
            "\n",
            "(27, 1)\n",
            "Levussaurus\n",
            "(27, 1)\n",
            "Heiaacosaurus\n",
            "(27, 1)\n",
            "Hyropameos\n",
            "(27, 1)\n",
            "Labadrophalons\n",
            "(27, 1)\n",
            "Xutiandosaurus\n",
            "(27, 1)\n",
            "Cabbus\n",
            "(27, 1)\n",
            "Toranisaurus\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ8ZSp6W1glP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv model_shakespeare_kiank_350_epoch.h5 models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE8Saf591obe",
        "colab_type": "code",
        "outputId": "028e6fba-2921-4aee-aa53-12ff70773345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls models/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_shakespeare_kiank_350_epoch.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97pbJoEn0yAo",
        "colab_type": "code",
        "outputId": "3fc118ad-656f-4ac8-c369-b981939bc913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
        "from keras.layers import LSTM\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from shakespeare_utils import *\n",
        "import sys\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading text data...\n",
            "Creating training set...\n",
            "number of training examples: 31412\n",
            "Vectorizing training set...\n",
            "Loading model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:350: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "  warnings.warn('Error in loading the saved optimizer '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlTEDF6y16EL",
        "colab_type": "code",
        "outputId": "8e8adbd0-3945-40a2-e9d2-2f95cb95301a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "31412/31412 [==============================] - 64s 2ms/step - loss: 2.7403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff4d22660b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yc4uFkR2KYA",
        "colab_type": "code",
        "outputId": "4b7085d2-585a-48d7-d02d-0f0c73a368f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "generate_output()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: It's a general rule in Python\n",
            "\n",
            "\n",
            "Here is your poem: \n",
            "\n",
            "It's a general rule in Pythons,\n",
            "which memund when i live wo dots of forse posceded,\n",
            "thou masd as faell's swilt lave shall-dude.\n",
            "of thou onthr usperind with me whet flol thy cread,\n",
            "that sturge my waited of wot thy bereud hist,\n",
            "whice in thi ubtisen the life herou seant,\n",
            "ye love nude hid wastantion me hath i be whoth from foold, thou babn,\n",
            "the foull, her tenrald wi owl time opforest and you,\n",
            "of to who mo cwlo'ct bes love to me w"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}