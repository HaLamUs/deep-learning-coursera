{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Neural machine translation with attention - v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "obekpM62HCNC",
        "colab_type": "code",
        "outputId": "64e8bff9-a35b-48a0-98a8-f86fb9464609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install Faker"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Faker in /usr/local/lib/python3.6/dist-packages (4.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from Faker) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->Faker) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6AiN3VC5iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b32W5U4LFSTC",
        "colab_type": "code",
        "outputId": "f80cd02d-3e05-43e1-c618-bbf2ea43de51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 22260.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtxAiQ6AFs3v",
        "colab_type": "code",
        "outputId": "93a6df2f-8359-4abe-8bad-00efea0d9de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "dataset[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('14 january 1975', '1975-01-14'),\n",
              " ('wednesday february 15 2017', '2017-02-15'),\n",
              " ('wednesday december 22 1999', '1999-12-22'),\n",
              " ('6/27/90', '1990-06-27'),\n",
              " ('sunday february 14 1999', '1999-02-14'),\n",
              " ('wednesday october 11 1978', '1978-10-11'),\n",
              " ('03 may 1990', '1990-05-03'),\n",
              " ('12 04 95', '1995-04-12'),\n",
              " ('monday april 21 1986', '1986-04-21'),\n",
              " ('monday december 30 1996', '1996-12-30')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42VzZ4TaJp-F",
        "colab_type": "code",
        "outputId": "91f01f80-89ec-42f3-89e5-498377f09180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "human_vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '.': 1,\n",
              " '/': 2,\n",
              " '0': 3,\n",
              " '1': 4,\n",
              " '2': 5,\n",
              " '3': 6,\n",
              " '4': 7,\n",
              " '5': 8,\n",
              " '6': 9,\n",
              " '7': 10,\n",
              " '8': 11,\n",
              " '9': 12,\n",
              " '<pad>': 36,\n",
              " '<unk>': 35,\n",
              " 'a': 13,\n",
              " 'b': 14,\n",
              " 'c': 15,\n",
              " 'd': 16,\n",
              " 'e': 17,\n",
              " 'f': 18,\n",
              " 'g': 19,\n",
              " 'h': 20,\n",
              " 'i': 21,\n",
              " 'j': 22,\n",
              " 'l': 23,\n",
              " 'm': 24,\n",
              " 'n': 25,\n",
              " 'o': 26,\n",
              " 'p': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'y': 34}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93rjdVZOKz6q",
        "colab_type": "code",
        "outputId": "da4f20b6-6b76-4b07-80de-71b95f58dc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "machine_vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-': 0,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdMMHRBZK3aZ",
        "colab_type": "code",
        "outputId": "c1f1aef4-9d70-4fe5-bbeb-0e7b47c11eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR98WPUnLDhf",
        "colab_type": "code",
        "outputId": "a70988f0-4fe3-456f-8bd9-91b76547b6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: 14 january 1975\n",
            "Target date: 1975-01-14\n",
            "\n",
            "Source after preprocessing (indices): [ 4  7  0 22 13 25 31 13 28 34  0  4 12 10  8 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10  8  6  0  1  2  0  2  5]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPWcz9cgLGen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX-XSavqOWmD",
        "colab_type": "code",
        "outputId": "2732e24b-1283-4dfd-c825-f462f54319c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "inp1 = Input(shape=(10,32))\n",
        "inp2 = Input(shape=(10,32))\n",
        "cc1 = Concatenate([inp1, inp2]) \n",
        "print(cc1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.layers.merge.Concatenate object at 0x7f9573d09390>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTlkJ-qQl96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: one_step_attention\n",
        "\n",
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    r_s_prev = repeator(s_prev)\n",
        "\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    c_a_and_s_prev = concatenator([a, r_s_prev])\n",
        "\n",
        "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
        "    e = densor(c_a_and_s_prev) # y = ax + b \n",
        "\n",
        "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    # alpha(t1,t2) = softmax(e(t1,t2))/sum softmax(e(t1,t2))\n",
        "    alphas = activator(e) # apply activation for output \n",
        "\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnYUg4iI_Uxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 64\n",
        "n_s = 128\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uISZF-2_f40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n",
        "    print(X)\n",
        "    print(s)\n",
        "    \n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for step in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a,s) # At step t, given all the hidden states of the Bi-LSTM \n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        hidden_state, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(hidden_state)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "\n",
        "    model = Model([X, s0, c0], outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYnowjmMCu8o",
        "colab_type": "code",
        "outputId": "0383180a-c218-46cd-8795-19f47ac859c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_43:0\", shape=(?, 30, 37), dtype=float32)\n",
            "Tensor(\"s0_14:0\", shape=(?, 128), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEGuwpQCCv6k",
        "colab_type": "code",
        "outputId": "9264d8e9-77b7-4e31-f2c5-033f729d0395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_43 (InputLayer)           (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_14 (Bidirectional (None, 30, 128)      52224       input_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_2 (RepeatVector)  (None, 30, 128)      0           s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 s0[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 30, 256)      0           bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[27][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[28][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[29][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[30][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[31][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[32][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[33][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[34][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[35][0]           \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 repeat_vector_2[36][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        257         concatenate_9[24][0]             \n",
            "                                                                 concatenate_9[25][0]             \n",
            "                                                                 concatenate_9[26][0]             \n",
            "                                                                 concatenate_9[27][0]             \n",
            "                                                                 concatenate_9[28][0]             \n",
            "                                                                 concatenate_9[29][0]             \n",
            "                                                                 concatenate_9[30][0]             \n",
            "                                                                 concatenate_9[31][0]             \n",
            "                                                                 concatenate_9[32][0]             \n",
            "                                                                 concatenate_9[33][0]             \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[24][0]                   \n",
            "                                                                 dense_2[25][0]                   \n",
            "                                                                 dense_2[26][0]                   \n",
            "                                                                 dense_2[27][0]                   \n",
            "                                                                 dense_2[28][0]                   \n",
            "                                                                 dense_2[29][0]                   \n",
            "                                                                 dense_2[30][0]                   \n",
            "                                                                 dense_2[31][0]                   \n",
            "                                                                 dense_2[32][0]                   \n",
            "                                                                 dense_2[33][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, 1, 128)       0           attention_weights[24][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[25][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[26][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[27][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[28][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[29][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[30][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[31][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[32][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "                                                                 attention_weights[33][0]         \n",
            "                                                                 bidirectional_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_2[23][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_2[24][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[23][2]                    \n",
            "                                                                 dot_2[25][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[24][2]                    \n",
            "                                                                 dot_2[26][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[25][2]                    \n",
            "                                                                 dot_2[27][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[26][2]                    \n",
            "                                                                 dot_2[28][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[27][2]                    \n",
            "                                                                 dot_2[29][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[28][2]                    \n",
            "                                                                 dot_2[30][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[29][2]                    \n",
            "                                                                 dot_2[31][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[30][2]                    \n",
            "                                                                 dot_2[32][0]                     \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 lstm_1[31][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           1419        lstm_1[23][0]                    \n",
            "                                                                 lstm_1[24][0]                    \n",
            "                                                                 lstm_1[25][0]                    \n",
            "                                                                 lstm_1[26][0]                    \n",
            "                                                                 lstm_1[27][0]                    \n",
            "                                                                 lstm_1[28][0]                    \n",
            "                                                                 lstm_1[29][0]                    \n",
            "                                                                 lstm_1[30][0]                    \n",
            "                                                                 lstm_1[31][0]                    \n",
            "                                                                 lstm_1[32][0]                    \n",
            "==================================================================================================\n",
            "Total params: 185,484\n",
            "Trainable params: 185,484\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me191MajI36B",
        "colab_type": "code",
        "outputId": "fc9254fe-9e01-46f9-b585-5212b1c84d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnImL1gSJHbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHe1KRadJgDV",
        "colab_type": "code",
        "outputId": "7c4545fd-7a5a-47ac-9c3f-97e8d52ec68b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "10000/10000 [==============================] - 22s 2ms/step - loss: 20.0886 - dense_3_loss: 2.7280 - dense_3_acc: 0.1059 - dense_3_acc_1: 0.4525 - dense_3_acc_2: 0.2590 - dense_3_acc_3: 0.1091 - dense_3_acc_4: 0.4671 - dense_3_acc_5: 0.2769 - dense_3_acc_6: 0.0824 - dense_3_acc_7: 0.6703 - dense_3_acc_8: 0.1230 - dense_3_acc_9: 0.0558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f956ec81a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZp9Dq4yKTlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5OuCddmKVaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "for example in EXAMPLES:\n",
        "    \n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    prediction = model.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    \n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgUf4icFMswt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday April 08 1993\", num = 6, n_s = 128)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}